<html>
<head>
  <title>Evernote Export</title>
  <basefont face="Tahoma" size="2" />
  <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
  <meta name="exporter-version" content="Evernote Windows/303788 (ko-KR, DDL); Windows/10.0.14393 (Win64);"/>
  <style>
    body, td {
      font-family: Tahoma;
      font-size: 10pt;
    }
  </style>
</head>
<body>
<a name="550"/>

<div>
<span><div><span style="font-size: 15px;"><b><span style="font-family: '맑은 고딕';"><span style="background-color: rgb(255, 250, 165);-evernote-highlight:true;">Deep learning</span><br/></span></b></span></div><div><hr/></div><div><span style="font-family: '맑은 고딕';"> A solution to the intuitive problems like recognizing spoken words or faces in images. It allows computers to learn from experience and understand the world in terms of <b>a hierarchy of concepts, with each concept defined in terms of its relation to simpler concepts</b>. </span><span style="font-family: '맑은 고딕';">... If we draw a graph showing how these concepts are built on top of each other, the graph is deep, with many layers. For this reason, we call this approach to AI deep learning.</span></div><div><span style="font-family: '맑은 고딕';"><br/></span></div><div><span style="font-family: '맑은 고딕';"><br/></span></div><div><span style="background-color: rgb(255, 250, 165);-evernote-highlight:true;"><span style="font-size: 15px;"><b><span style="font-family: '맑은 고딕';">Knowledge base approach (hard-coding)</span></b></span></span></div><div><hr/></div><div><span style="font-family: '맑은 고딕';"> An approach seeking to hard-code knowledge about the world in formal languages. A computer can reason about statements in these formal languages automatically using logical inference rules</span><span style="font-family: '맑은 고딕';">. These atatements are entered by human supervisors. It is an unwieldy process. People struggle to devise formal rules with enough complexity to accuately descibe the world.</span></div><div><span style="font-family: '맑은 고딕';"> (e.g. Cyc - 'FredWhileShaving' contained electircal parts.)</span><span style="font-family: '맑은 고딕';"> </span></div><div><span style="font-family: '맑은 고딕';"><br/></span></div><div><span style="font-family: '맑은 고딕';"><br/></span></div><div><span style="background-color: rgb(255, 250, 165);-evernote-highlight:true;"><span style="font-size: 15px;"><b><span style="font-family: '맑은 고딕';">Machine Learning</span></b></span></span></div><div><hr/></div><div><span style="font-family: '맑은 고딕';"> The ability that AI systems <b>acquire their own knowledge</b>, <b>by extracting patterns from raw data.</b> This allowed computers to tackle problems involving knowledge of the real world and make decisions that appear subjective.</span></div><div><span style="font-family: '맑은 고딕';"> (e.g. Logistic regression - cesarean delivery decision, Naive Bayes - separating legitimate e-mail from spam e-mail.)</span></div><div><span style="font-family: '맑은 고딕';"><br/></span></div><div><span style="font-family: '맑은 고딕';"><br/></span></div><div><span style="background-color: rgb(255, 250, 165);-evernote-highlight:true;"><span style="font-size: 15px;"><b><span style="font-family: '맑은 고딕';">Representation learning</span></b></span></span></div><div><hr/></div><div><span style="font-family: '맑은 고딕';"> The performance of simple machine learning algorithms depends heavily on the representation of the data they are given. ... However, for many tasks, it tis difficult to know what features should by extracted. ... One solution to this problem is <b>to use machine learning to discover not only the mapping from representation to output but also the representation itself</b>. This approach is known as representation learning.  </span><span style="font-family: '맑은 고딕';">(e.g. Autoencoder.) </span></div><div><span style="font-family: '맑은 고딕';"><br/></span></div><div><span style="font-family: '맑은 고딕';"> <b>Deep learning</b> solves the difficulty to obtain a representation as to solve the original problem by introducing representations that are expressed in terms of other, simpler representations.</span></div><div><span style="font-family: '맑은 고딕';"> (e.g. Feedforward deep network or multilayer perceptron(MLP) )</span></div><div><span style="font-family: '맑은 고딕';"><br/></span></div><div><span style="font-family: '맑은 고딕';"><br/></span></div><div><img src="Evernote.enex_files/Image.png" type="image/png" style="height:auto;" width="520"/></div><div><img src="Evernote.enex_files/Image [1].png" type="image/png" style="height:auto;" width="518"/></div><div><br/></div><div><br/></div><div><span style="font-size: 15px;"><b>1.2 Historical Trends in Deep Learning</b></span></div><div><b><span style="font-size: 15px;"><br/></span></b></div><div><b>  1.2.1 The Many Names and Changing Fortunes of Neural Networks<br/></b></div><div><hr/></div><div><br/></div><div> The first wave (1940s - 1960s)</div><div><b><span style="background-color: rgb(255, 250, 165);-evernote-highlight:true;"><span style="font-size: 15px;"> Cybernetics</span>.</span></b></div><div><b><br/></b></div><ul><li><b>Simple linear models</b>, the perceptron and ADALINE, motivated from a neuroscientific perspective. <br/><br/></li><li>The <b>McCulloch-Pitts Neuron</b> <span style="color: rgb(121, 121, 121);">(McCulloch and Pitts, 1943)</span> was an <b>early model of brain function</b>.
<ul><li>Recognize two different categories of inputs by testing whether f(x, w) is positive or negative.</li><li>the weights needed to be set by the human operator.<br/><br/></li></ul></li><li>In the 1950s, <b>the perceptron</b> <span style="color: rgb(121, 121, 121);">(Rosenblatt, 1958, 1962)</span> became the <b>first model that could learn the weights</b> defining the categories given examples of inputs from each category.<br/><br/></li><li><b>The adaptive linear element</b> (ADALINE) <span style="color: rgb(121, 121, 121);">- Widrow and Hoff, 1960 -</span> could learn to predict dates from data.</li></ul><div><br/></div><ul><li><u>Linear models have many limitations. (e.g. Inability to learn XOR function)</u></li></ul><div><u><br/></u></div><ul><li>We are able to draw some &quot;rough&quot; guidelines from neuroscience (the way brain works.).
<ul><li>The Neocognitron <span style="color: rgb(121, 121, 121);">(Fukushima, 1980)</span> introduced a powerful model architecture for processing images that was inspired by the structure of the mammalian visual system and later became the basis for the modern convolutional network <span style="color: rgb(121, 121, 121);">(LeCun et al., 1998b)</span></li><li>Most neural networks today are based on a model neuron called <b>the rectified linear unit</b>.</li></ul></li></ul><div><br/></div><div><br/></div><div> The Second wave (1980s - 1990s)</div><div><span style="background-color: rgb(255, 250, 165);-evernote-highlight:true;"><span style="font-size: 15px;"><b> Connectionism</b></span></span></div><div><br/></div><ul><li>Called connectionism or parallel distributed processing <span style="color: rgb(121, 121, 121);">(Rumelhart et al., 1986c; McClelland et al., 1995)</span>.</li></ul><div><br/></div><ul><li>Connectionism arose in the context of congnitive science.<br/><br/></li><li>The central idea is that a large number of simple computational units can achieve intelligent behavior when networked together. -&gt; <b>hidden units</b> in computational models.<br/><br/></li><li><b>Distributed representation</b> <span style="color: rgb(121, 121, 121);">(Hinton et al., 1986)<br/><br/></span><ul><li>Each input to a system should be represented by many features, and each feature should be involved in the representation of many possible inputs. <br/><br/><ul><li>e.g. Objects options: cars, trucks, birds &amp; Color options: red, green, blue.</li><li>With a distributed representation it needs 6 neurons instead of 9, the possible combinations.</li><li>The neuron describing redness is able to learn about redness from images of cars, trucks and birds, not only from images on one specific category of objects.</li></ul></li></ul></li></ul><div><br/></div><ul><li><b>Back-propagation<br/><br/></b><ul><li>To train deep neural networks with internal representations and the popularization of the back-propagation algorithm <span style="color: rgb(121, 121, 121);">(Rumelhart et al., 1986a; LeCun, 1987)<br/><br/></span></li><li>Currently the dominant approach to training deep models.<br/><br/></li></ul></li><li><b>Long short-term memory</b> or <b>LSTM network<br/><br/></b><ul><li>resolves some of mathematical difficulties in modeling long sequences with neural networks<br/><span style="color: rgb(121, 121, 121);">(Hochreiter and Schmidhuber (1997)<br/><br/></span></li></ul></li><li><b>Canadian Institute for Advanced Research (CIFAR)<br/><br/></b><ul><li>took Neural Computation and Adaptive Perception <b>(NCAP)</b> research initiative.</li><li>NCAP led by Geoffrey Hinton at University of Toronto and others.</li><li>The CIFAR NCAP research initiative had a multi-disciplinary nature that also included nueroscientists and experts in human and computer vision.</li></ul></li></ul><div><br/></div><div><br/></div><div>The Third wave ( Since 2006 ~ ) </div><div><span style="background-color: rgb(255, 250, 165);-evernote-highlight:true;"><span style="font-size: 15px;"><b>Deep learning</b></span></span></div><div><b><span style="font-size: 15px;"><span style="background-color: rgb(255, 250, 165);-evernote-highlight:true;"><br/></span></span></b></div><ul><li>Using a strategy called <b>greedy layer-wise pretraining</b> <span style="color: rgb(121, 121, 121);">(Hinton et al., 2006)</span></li><li>the use of the term &quot;deep learning&quot; to emphasize that researchers were now able to train deeper neural networks than had been possible before.</li><li>outperformed competing AI systems based on other machine learning technologies as well as hand-designed functionality.</li></ul><div><br/></div><div><br/></div><div><br/></div><div><b> 1.2.2 Increasing Dataset Sizes</b></div><div><hr/></div><div><br/></div><ul><li>The learning algorithms reaching human performance on complex tasks today are nearly identical to the learning algorithms that struggled to solve toy problems in the 1980s.<br/><br/></li><li>The most important new development is that today we can provide these algorithms with the resources they need to succeed.</li></ul><div><b>     </b></div><div><b>     <img src="Evernote.enex_files/Image [2].png" type="image/png" style="height:auto;" width="589"/></b></div><div>           <img src="Evernote.enex_files/Image [3].png" type="image/png" style="height:auto;" width="572"/></div><ul><li>This trend is driven by the increasing digitization of society.</li><li>The age of &quot;Big Data&quot; has made machine learning much easier.</li></ul><div><br/></div><div><b><br/></b></div><div><b><br/></b></div><div><b>1.2.3 Increasing Model Sizes</b></div><div><hr/></div><div><b><br/></b></div><ul><li>Since the introduction of hidden units, artificial neural networks have doubled in size roughly every 2.4 years.</li></ul><div><br/></div><ul><li>Deu to the availability of faster CPUs, the advent of general purpose GPUs, faster network connectivity and better software infrastructure for distributed computing.</li></ul><div><b><br/></b></div><div><b><br/></b></div><div><b><br/></b></div><div><b> 1.2.4 Increasing Accuracy, Complexity and Real-World Impact</b></div><div><hr/></div><div><b><br/></b></div><ul><li><b>Recurrent neural networks</b>, such as the <b>LSTM</b> sequence model, are now used to model relationships between <i>sequences</i> and other <i>sequences</i> rather than just fixed inputs. This <b>sequence-to-sequence learning</b> seems to be on the cusp of revolutionizing another application: machine translation <span style="color: rgb(121, 121, 121);">(Sutskever et al., 2014; Bahdanau et al., 2015)</span>.</li></ul><div><br/></div><ul><li>The introduction of neural <b>Turing machine</b>s, Self-programming <span style="color: rgb(121, 121, 121);">(Graves et al., 2014a)</span>.</li></ul><div><br/></div><ul><li><b>Reinforcement learning<br/></b><ul><li>In the context of reinforcement learning, an autonomous agent must learn to perform a task by <b>trial and error</b>, <b>without any guidance from the human</b> operator.</li></ul></li></ul><div><br/></div><ul><li>Advances in Software infrastructure. 
<ul><li>Torch, Caffe, MXNet, TensorFlow, and so on... </li></ul></li></ul><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div><span style="font-family: '맑은 고딕';"> </span></div></span>
</div></body></html> 